#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/*
 * Batch normalization layer.
 * 
 */
forward = function(matrix[double] X, matrix[double] ema_mean, matrix[double] ema_var,
                   int C, int H, int W,
                   matrix[double] gamma, matrix[double] beta,
                   double ma_fraction, double eps, string mode)
          return (matrix[double] out, matrix[double] ema_mean_out, matrix[double] ema_var_out) {

   ema_mean_out = ema_mean
   ema_var_out = ema_var

   k = 1 + as.scalar(nrow(ema_mean) != as.matrix(C*H*W)) * (H*W-1)
   ones = matrix(1, rows=1, cols=k)
   m = matrix(ema_mean_out %*% ones, rows=1, cols=C*H*W)
   v = matrix(ema_var_out %*% ones, rows=1, cols=C*H*W)
   gamma = matrix(gamma %*% ones, rows=1, cols=C*H*W)
   beta = matrix(beta %*% ones, rows=1, cols=C*H*W)
   
   out = (X - m) / sqrt(v + eps)
   out = out*gamma + beta
}


forward_old = function(matrix[double] X, matrix[double] ema_mean, matrix[double] ema_var,
                   int C, int H, int W, 
                   matrix[double] gamma, matrix[double] beta, 
                   double ma_fraction, double eps,
                   string mode) 
          return (matrix[double] out, matrix[double] ema_mean_out, matrix[double] ema_var_out) {
  /*
   * Computes the forward pass for a batch normalization layer. Uses an exponential moving average.
   *
   * See S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." arXiv preprint arXiv:1502.03167 (2015).
   *
   * Inputs:
   * - X: Input data matrix, of shape (N, C*H*W).
   * - ema_mean: exponential moving average for mean from previous iteration (of dimension either [C, 1] or [C*H*W, 1]).
   * - ema_var: exponential moving average for variance from previous iteration (of dimension either [C, 1] or [C*H*W, 1]).
   * - C: Number of input channels
   * - H: Input height.
   * - W: Input width.
   * - gamma: multiplier
   * - beta: bias 
   * - ma_fraction: moving average fraction (typical value: 0.999)
   * - eps: value to be added to variance (typical value: 1e-5)
   * - mode: can be 'train' or 'test'
   * 
   * Outputs:
   * - out: output matrix, of shape (N, C*H*W).
   * - ema_mean_out: new exponential moving average for mean (of dimension either [C, 1] or [C*H*W, 1]).
   * - ema_var_out: new exponential moving average for variance (of dimension either [C, 1] or [C*H*W, 1]).
   */
	if(mode == 'test') {
	  # TEST
	  ema_mean_out = ema_mean
	  ema_var_out = ema_var
	}
	else {
	  # TRAIN
	  if(nrow(ema_mean) == C*H*W) {
	    # Regular normalization
	    ema_mean_out = ma_fraction*colMeans(X) + (1-ma_fraction)*ema_mean
	    ema_var_out = ma_fraction*colVars(X) + (1-ma_fraction)*ema_var
	  }
	  else {
	    # Per-channel batch normalization 
	    # Simple approximation
	    batch_mean = matrix(colMeans(X), rows=C, cols=H*W)
	    
	    # Compute per-channel mean and variance (using exponential moving average)
	    ema_mean_out = ma_fraction*rowMeans(batch_mean) + (1-ma_fraction)*ema_mean
	    ema_var_out = ma_fraction*rowVars(batch_mean) + (1-ma_fraction)*ema_var
	  }
	}
	if(nrow(ema_mean) == C*H*W) {
	  m = ema_mean_out
	  v = ema_var_out
	}
	else {
	  # Could be replaced by bias_add
	  ones = matrix(1, rows=1, cols=H*W)
	  m = matrix(ema_mean_out %*% ones, rows=1, cols=C*H*W)
	  v = matrix(ema_var_out %*% ones, rows=1, cols=C*H*W)
	  gamma = matrix(gamma %*% ones, rows=1, cols=C*H*W)
	  beta = matrix(beta %*% ones, rows=1, cols=C*H*W)
	}
	out = (X - m) / sqrt(v + eps)
	out = out*gamma + beta
}

# Sum per channel (ouput shape: [C, 1]): rowSums(matrix(colSums(X), rows=C, cols=HW))

backward = function(matrix[double] dout, matrix[double] X, matrix[double] out, matrix[double] ema_mean, matrix[double] ema_var,
                   int C, int H, int W, matrix[double] gamma, double eps) 
          return (matrix[double] dX, matrix[double] dgamma, matrix[double] dbeta) {
  /*
   * Computes the backward pass for a batch normalization layer. Uses an exponential moving average.
   *
   * See S. Ioffe and C. Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." arXiv preprint arXiv:1502.03167 (2015).
   *
   * Inputs:
   * - dout: Derivatives from upstream, of shape (N, C*H*W).
   * - X: Input data matrix, of shape (N, C*H*W).
   * - ema_mean: exponential moving average for mean from previous iteration (of dimension either [C, 1] or [C*H*W, 1]).
   * - ema_var: exponential moving average for variance from previous iteration (of dimension either [C, 1] or [C*H*W, 1]).
   * - C: Number of input channels
   * - H: Input height.
   * - W: Input width.
   * - gamma: multiplier
   * - eps: value to be added to variance (typical value: 1e-5)
   * 
   * Outputs:
   * - dX: Gradient wrt X, of shape (N, C*H*W).
   * - dgamma: Gradient wrt gamma
   * - dbeta: Gradient wrt beta
   * 
   */

	# TODO: Double-check this !!!
	if(nrow(ema_mean) == C*H*W) {
	  dbeta = colSums(dout)
	  dgamma = colSums(out*dout)
	  m = ema_mean
	  v = ema_var
	}
	else {
	  dbeta = rowSums(matrix(colSums(dout), rows=C, cols=H*W)) 
	  dgamma = rowSums(matrix(colSums(out*dout), rows=C, cols=H*W))
	  ones = matrix(1, rows=1, cols=H*W)
	  gamma = matrix(gamma %*% ones, rows=1, cols=C*H*W)
	  m = matrix(ema_mean %*% ones, rows=1, cols=C*H*W)
	  v = matrix(ema_var %*% ones, rows=1, cols=C*H*W)
	}
	# See http://cthorey.github.io./backpropagation/
	term1 = colSums(dout) # sum_k dL / dy_{kj}
	N = nrow(X)
	out = (X - m) / sqrt(v + eps)
	dX = ( N*dout - term1 - out * colSums(dout*(X - m))) * (1/N) * gamma * sqrt(v + eps)
}

init = function(int size) return (matrix[double] ema_mean, matrix[double] ema_var, matrix[double] gamma, matrix[double] beta) {
  /*
   * Initialize the parameters of this layer.
   *
   * Inputs:
   * - size: can be either number of channels C or C*H*W (where H is input height and W is input width)
   * 			size = C  ===> per-channel batch normalization (used in ResNet)
   * 
   * Outputs:
   * - ema_mean: initial exponential moving average for mean (of dimension either [C, 1] or [C*H*W, 1])
   * - ema_var: initial exponential moving average for variance (of dimension either [C, 1] or [C*H*W, 1])
   */
   # Per-channel batch normalization
   ema_mean = matrix(0, rows=size, cols=1)
   ema_var = matrix(0, rows=size, cols=1)
   gamma = matrix(1, rows=size, cols=1)
   beta = matrix(0, rows=size, cols=1)
}
